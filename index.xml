
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
 <channel>
   <title>Infrastructure Guy</title>
   <link>https://glaspie.us/</link>
   <description>Recent content on Infrastructure Guy</description>
   <generator>Hugo -- gohugo.io</generator>
   <copyright>Copyright &amp;copy; 2018 - Joseph Glaspie</copyright>
   <lastBuildDate>Fri, 28 Jan 2022 10:47:37 -0600</lastBuildDate>
   
       <atom:link href="https://glaspie.us/index.xml" rel="self" type="application/rss+xml" />
   
   
     <item>
       <title>Terraform Twilio Golang Oh-my</title>
       <link>https://glaspie.us/posts/terraform-twilio-golang-ohmy/</link>
       <pubDate>Fri, 28 Jan 2022 10:47:37 -0600</pubDate>
       
       <guid>https://glaspie.us/posts/terraform-twilio-golang-ohmy/</guid>
       <description>&lt;p&gt;Want to get your feet wet with a little Terraform, Twilio, EKS, and Golang? I created a super simple app that will allow you to text the number of friends you are with and reply back with a fun activity.&lt;/p&gt;&lt;p&gt;The app is built on top of AWS&amp;rsquo;s Kubernetes (EKS). I wanted an easy setup that I could quickly destroy and not incure costs beyond the Free AWS Tier so leveraged Terraform to do so.&lt;/p&gt;&lt;p&gt;The Bored API doesn&amp;rsquo;t require an authentication so it&amp;rsquo;s super easy to make rest calls against it. The reason behind using Terraform was so that I could tear down the infrastructure and build it back identically. The only &amp;ldquo;gotcha&amp;rdquo; I found was that the Terraform wouldn&amp;rsquo;t remove all of the service accounts created so you gotta clear those out with a script. I&amp;rsquo;m happy to throw something together if you hit me up on LinkedIn.&lt;/p&gt;&lt;p&gt;Here&amp;rsquo;s the tutorial if you want to give it a shot or the &lt;a href=&#34;https://github.com/josephglaspie/golangBored&#34;&gt;repo&lt;/a&gt; if you&amp;rsquo;d rather skip my rambling and go strait to the code.&lt;/p&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;curl --location --request POST &#39;http://k8s-bored-ingressb-a7cccb6576-1355983709.us-east-2.elb.amazonaws.com/bored&#39; \--header &#39;Content-Type: text/plain&#39; \--data-raw &#39;5&#39;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;and return results with something fun to do.&lt;/p&gt;&lt;h3 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h3&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;#install Gobrew install go#install Terraformbrew install terraform#install Dockerbrew cask install docker#install minikubecurl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-darwin-amd64sudo install minikube-darwin-amd64 /usr/local/bin/minikube#install Kubectlcurl -LO &amp;quot;https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/darwin/amd64/kubectl&amp;quot;&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&#34;to-run-locally&#34;&gt;To run locally&lt;/h4&gt;&lt;p&gt;&lt;code&gt;go run app.go&lt;/code&gt;&lt;/p&gt;&lt;h4 id=&#34;push-to-docker-hub&#34;&gt;Push to Docker Hub&lt;/h4&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;APP_TAG=josephglaspie/bored:v0.0.5docker build . -t $APP_TAGdocker push $APP_TAG&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&#34;to-deploy-to-minikube&#34;&gt;To deploy to minikube&lt;/h4&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;kubectl apply -f deployments/mini-deployment.yaml# The following will return the portminikube service web&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&#34;make-available-online-with-ngrok&#34;&gt;Make available online with ngrok&lt;/h4&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;ngrok http http://127.0.0.1:56952&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Take the return from ngrok and add it to the twilio webhook&lt;/p&gt;&lt;h4 id=&#34;eks&#34;&gt;EKS&lt;/h4&gt;&lt;ul&gt;&lt;li&gt;Used eks module: &lt;a href=&#34;https://registry.terraform.io/modules/terraform-aws-modules/eks/aws/latest&#34;&gt;https://registry.terraform.io/modules/terraform-aws-modules/eks/aws/latest&lt;/a&gt;&lt;/li&gt;&lt;li&gt;Create main.tf as seen in this repo and tfvars.tf (.gitignore) with the following:&lt;/li&gt;&lt;/ul&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;provider &amp;quot;aws&amp;quot; {  region     = &amp;quot;us-east-2&amp;quot;  access_key = &amp;quot;&amp;quot;  secret_key = &amp;quot;&amp;quot;}&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;&lt;li&gt;You can get your access_key and secret_key from the security credential part of the console&lt;a href=&#34;https://console.aws.amazon.com/iam/home?region=us-east-2#/security_credentials&#34;&gt;https://console.aws.amazon.com/iam/home?region=us-east-2#/security_credentials&lt;/a&gt;&lt;/li&gt;&lt;li&gt;Acess Keys &amp;gt; Create new key&lt;/li&gt;&lt;/ul&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;cd terraformterraform initterraform planterraform apply #Go make some tea or coffee this will take around 10 minutes to build outcd ..# Connect your kubectl to the clusteraws configure #paste your keys into the correct placesaws eks --region us-east-2 update-kubeconfig --name mycluster# Will return something like: Updated context arn:aws:eks:us-east-2:244172242562:cluster/mycluster in /Users/name/.kube/config&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;ingress&#34;&gt;Ingress&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;&lt;a href=&#34;https://docs.aws.amazon.com/eks/latest/userguide/alb-ingress.html&#34;&gt;The Best Ingress Doc&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes-sigs/aws-load-balancer-controller&#34;&gt;AWS Ingress Github&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&#34;https://docs.aws.amazon.com/eks/latest/userguide/aws-load-balancer-controller.html&#34;&gt;AWS App Load Balancer&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&#34;https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.3/how-it-works/&#34;&gt;How AWS Load Balancer controller works&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;#Create IAM Policyaws iam create-policy \--policy-name AWSLoadBalancerControllerIAMPolicy \--policy-document file://iam_policy.json#Create service accountmyaccount=244172242562clustername=myclustereksctl create iamserviceaccount \--cluster=$clustername \--namespace=kube-system \--name=aws-load-balancer-controller \--attach-policy-arn=arn:aws:iam::$myaccount:policy/AWSLoadBalancerControllerIAMPolicy \--override-existing-serviceaccounts \--approveeksctl utils associate-iam-oidc-provider --region=us-east-2 --cluster=mycluster --approve# Install cert-manager to inject certificate configuration into the webhooks.kubectl apply \--validate=false \-f https://github.com/jetstack/cert-manager/releases/download/v1.5.4/cert-manager.yaml# Download and replace default cluster name with your owncurl -Lo deployments/loadbalancer.yaml https://github.com/kubernetes-sigs/aws-load-balancer-controller/releases/download/v2.3.0/v2_3_0_full.yaml# Apply ALB with correct cluster namekubectl apply -f deployments/loadbalancer.yaml# Check on ALB deploymentkubectl get deployment -n kube-system aws-load-balancer-controllerkubectl logs -n kube-system   deployment.apps/aws-load-balancer-controller&lt;/code&gt;&lt;/pre&gt;&lt;ol&gt;&lt;li&gt;Log into console and tage subnets with the following:Key – kubernetes.io/role/elbValue – 1&lt;/li&gt;&lt;li&gt;Also, download the sample app and change out the settings with your own AND add your subnets to the ingress annotation:&lt;/li&gt;&lt;/ol&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;curl -Lo deployments/web-deployment.yaml https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.3.0/docs/examples/2048/2048_full.yamlalb.ingress.kubernetes.io/subnets: subnet-0213058ab09c17cca, subnet-003c36f6f9c457619, subnet-00a17755b8471dbd5&lt;/code&gt;&lt;/pre&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;# Test connectionk get pods -n kube-system# Deploy app with service and ingressk apply -f deployments/web-deployment.yaml# Check ingresskubectl get ingress/ingress-bored -n bored#get external public ipk get svcNAME                       TYPE           CLUSTER-IP       EXTERNAL-IP                                                               PORT(S)          AGEkubernetes                 ClusterIP      10.100.0.1       &amp;lt;none&amp;gt;                                                                    443/TCP          10mweb                        NodePort       10.100.130.96    &amp;lt;none&amp;gt;                                                                    8080:31317/TCP   10sweb-service-cluster-ip     ClusterIP      10.100.231.227   &amp;lt;none&amp;gt;                                                                    8080/TCP         10sweb-service-loadbalancer   LoadBalancer   10.100.121.221   a93868263dcf34d12b029f8c080c8951-1996062109.us-east-2.elb.amazonaws.com   8080:30061/TCP   10sweb-service-nodeport       NodePort       10.100.143.240   &amp;lt;none&amp;gt;                                                                    80:31549/TCP     10scurl --location --request POST &#39;http://k8s-bored-ingressb-a7cccb6576-1355983709.us-east-2.elb.amazonaws.com/bored&#39; \--header &#39;Content-Type: text/plain&#39; \--data-raw &#39;5&#39;terraform destroy&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;references&#34;&gt;References:&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;a href=&#34;https://www.boredapi.com/&#34;&gt;Bored Api&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&#34;https://aws.amazon.com/eks/&#34;&gt;EKS&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&#34;https://www.twilio.com/sms&#34;&gt;Twilio&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description>
     </item>
   
     <item>
       <title>Azure Cloud - Makin&#39; It Rain</title>
       <link>https://glaspie.us/posts/azure-making-it-rain/</link>
       <pubDate>Tue, 04 Sep 2018 19:13:44 -0500</pubDate>
       
       <guid>https://glaspie.us/posts/azure-making-it-rain/</guid>
       <description>&lt;p&gt;&lt;img src=&#34;https://glaspie.us/rain.jpg&#34; alt=&#34;image alt text&#34;&gt;&lt;/p&gt;&lt;p&gt;One of the biggest advantages of deploying to the cloud is the ability to spin up large and spin up fast. You want a 64 x 1024 SQL box to run midnight batch jobs like no one&amp;rsquo;s business, you bet. How about a 128 x 4000 beast with massive SSD IOPs and storage, sure not a problem. Need to quickly scale these boxes horizontally at a specific time or trigger on CPU spikes?&lt;/p&gt;&lt;p&gt;You can do all of the above with just a few clicks in the portal, handful of lines in Powershell, and about 15 minutes cooking time. This is great if you have a need for near-instant massive compute power and a bottomless bank account. Unfortunately, we live in the real world which requires us to be good stewards of our resources. In this real world those servers cost real money: $7K and $27K per month respectively.&lt;/p&gt;&lt;p&gt;&lt;img src=&#34;https://glaspie.us/sizes.png&#34; alt=&#34;image alt text&#34;&gt;&lt;/p&gt;&lt;p&gt;We recently had a customer who worked with the vendor to create a POC to rival their on-prem environment. Our customer was told to make it happen and let the bean counters worry about the cost. With the vendor&amp;rsquo;s help they MADE IT RAIN. Massive VMs with 30 day full backups, ~100 unattached premium disks that were used once then left to rot like a 6th Street cigarette butt, and no way to understand how they were predicted to spend $1 million that year on infrastructure alone. Here&amp;rsquo;s how we got to the bottom of the issue.&lt;/p&gt;&lt;ol&gt;&lt;li&gt;First, we checked out the Azure Advisor which only recommended about $100/month in savings.&lt;/li&gt;&lt;li&gt;Then, we checked all of our backups and noticed that during the Microsoft Architect guided POC backups were setup to store a 30 days of full backups which we lowered to 3 days.&lt;/li&gt;&lt;li&gt;Finally, we created an Azure automation runbook &lt;a href=&#34;https://github.com/josephglaspie/Azure/blob/master/Get-UnusedAutomation.ps1&#34;&gt;Get-UnusedAutomation.ps1&lt;/a&gt; to gather all unattached disks and disks attacked to deallocated VMs and email our business analysis with the results weekly. Once we got approval to delete we imported the csv created in the previous step and used the following to remove the disks &lt;a href=&#34;https://github.com/josephglaspie/Azure/blob/master/Remove-UnusedDisk.ps1&#34;&gt;Remove-UnusedDisk.ps1&lt;/a&gt;.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;Using those strategies we were able to cut the monthly bill in half!&lt;/p&gt;&lt;p&gt;&lt;img src=&#34;https://glaspie.us/before.png&#34; alt=&#34;image alt text&#34;&gt;&lt;/p&gt;&lt;p&gt;Thanks for reading.&lt;/p&gt;</description>
     </item>
   
     <item>
       <title>Beware the Azure VM throttle</title>
       <link>https://glaspie.us/posts/beware-the-azure-vm-throttle/</link>
       <pubDate>Sat, 28 Jul 2018 00:00:00 +0000</pubDate>
       
       <guid>https://glaspie.us/posts/beware-the-azure-vm-throttle/</guid>
       <description>&lt;p&gt;One of my customers went live with their datawarehouse solution in Azure a few weeks back. Everything went smoothly until the analysts hit the boxes with more than they could handle and we had to rearchitect a new solution on the fly.&lt;/p&gt;&lt;p&gt;I wanted to explain what we saw from at a high-level and in common language without making my fellow engineers cringe too much.&lt;/p&gt;&lt;p&gt;When building a Virtual Machine (VM) in the cloud there are many specs to look at. We typically focus on things like HDD size and speed, amount of RAM and CPU counts and cores.&lt;/p&gt;&lt;p&gt;However, to keep all the VMs in the cloud from stomping all over each other Azure throttles the VMs much like an HOA throttles your neighbor from building a body shop on their front yard.&lt;/p&gt;&lt;p&gt;The machines we are using are spec’d for High Memory usage but have a cached Disk IO of 512MB/s total.&lt;/p&gt;&lt;p&gt;&lt;img src=&#34;https://glaspie.us/Picture1.png&#34; alt=&#34;image alt text&#34;&gt;&lt;/p&gt;&lt;p&gt;…This means that it only takes two 2TB disks at full utilization (250MB/s) to hit the Azure VM throttle of 512MB as highlighted above.&lt;/p&gt;&lt;p&gt;&lt;img src=&#34;https://glaspie.us/disklimits.png&#34; alt=&#34;image alt text&#34;&gt;&lt;/p&gt;&lt;p&gt;Our VM has multiple &amp;gt; 2TB disks attached with SQL running full speed. The CPU and RAM on the server are underutilized but the transaction times on the disks are at &amp;gt; 1000ms instead of 15-20 in a healthy non-throttled machine. We have validated this with PerfMon the local windows performance counter tool.&lt;/p&gt;&lt;p&gt;Currently, the L series VMs that have no storage throttles are available in East Us 2 where our subscription lives.&lt;/p&gt;&lt;h2 id=&#34;tldr&#34;&gt;TLDR&lt;/h2&gt;&lt;p&gt;We have a pontoon boat (optimized for memory) and even though we keep attaching motors and props the shape of the hull (VM) will only allow it to go about 15 MPHs. If you want it to go faster need to get a boat with a streamlined hull.&lt;/p&gt;&lt;h2 id=&#34;references&#34;&gt;References:&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;a href=&#34;https://docs.microsoft.com/en-us/azure/virtual-machines/windows/sizes-memory#esv3-series&#34;&gt;Azure VMs&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&#34;https://docs.microsoft.com/en-us/azure/virtual-machines/windows/premium-storage#scalability-and-performance-targets&#34;&gt;Azure Disks&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&#34;https://blogs.technet.microsoft.com/xiangwu/2017/05/14/azure-vm-storage-performance-and-throttling-demystify/&#34;&gt;On Azure VM Throttling&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</description>
     </item>
   
     <item>
       <title>My First Post</title>
       <link>https://glaspie.us/posts/my-new-post/</link>
       <pubDate>Sat, 28 Jul 2018 00:00:00 +0000</pubDate>
       
       <guid>https://glaspie.us/posts/my-new-post/</guid>
       <description>&lt;p&gt;Hello world this is my first post! I&amp;rsquo;ve written a couple of blogs for work and wanted to try out &lt;a href=&#34;https://gohugo.io/&#34;&gt;Hugo&lt;/a&gt;.&lt;/p&gt;</description>
     </item>
   
 </channel>
</rss>
